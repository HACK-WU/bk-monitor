# Development Guide

<cite>
**Referenced Files in This Document**   
- [check_commit_message.py](file://scripts/pre-commit/check_commit_message.py#L1-L61)
- [pre-push.py](file://scripts/pre-commit/pre-push.py#L1-L182)
- [preci.py](file://scripts/pre-commit/preci.py#L1-L25)
- [preci.sh](file://scripts/pre-commit/preci.sh#L1-L20)
- [pyproject.toml](file://pyproject.toml#L1-L63)
- [settings.py](file://bkmonitor/settings.py#L1-L73)
- [README.md](file://README.md#L1-L51)
- [drf_resource模块的使用.md](file://wiki/drf_resource模块的使用.md#L1-L480) - *Added documentation on drf_resource module usage*
- [time_series.py](file://bkmonitor/metadata/models/custom_report/time_series.py#L1-L1485) - *Added detailed comments explaining the design intent of the time series model*
- [base.py](file://bkmonitor/metadata/models/custom_report/base.py#L1-L523) - *Base class implementation for the time series model*
- [aidev_interface.py](file://ai_agent/core/aidev_interface.py#L1-L215) - *Core interface for AI agent functionality*
- [local_command_handler.py](file://ai_agent/services/local_command_handler.py#L1-L119) - *Local command processing for AI agent*
- [metrics_reporter.py](file://ai_agent/services/metrics_reporter.py#L1-L418) - *Metrics reporting implementation for AI agent*
- [pytest.yml](file://bkmonitor/.github/workflows/pytest.yml#L1-L69) - *Updated CI/CD workflow configuration*
- [plugin.py](file://bkmonitor/packages/monitor_web/models/plugin.py#L510-L540) - *Custom metric refresh logic implementation*
</cite>

## Update Summary
**Changes Made**   
- Added new section **AI Agent Development Workflow** to document the new AI agent SDK integration and development practices.
- Added new section **Frontend Component Development Guidelines** to address the custom metrics view component fixes.
- Updated **CI/CD Process** section with details from the `pytest.yml` workflow configuration.
- Enhanced **Testing Strategy** section with information about the GitHub Actions CI pipeline.
- Added new **Metrics Reporting and Monitoring** section to document the AI agent metrics infrastructure.
- Updated **Contribution Process and Best Practices** with AI agent development considerations.
- Added references to new and updated files in the source tracking system.

## Table of Contents
1. [Introduction](#introduction)
2. [Project Structure](#project-structure)
3. [Code Style and Conventions](#code-style-and-conventions)
4. [Dependency Management and Build Configuration](#dependency-management-and-build-configuration)
5. [Pre-commit Hooks and Code Checks](#pre-commit-hooks-and-code-checks)
6. [Configuration Management](#configuration-management)
7. [Testing Strategy](#testing-strategy)
8. [CI/CD Process](#cicd-process)
9. [Contribution Process and Best Practices](#contribution-process-and-best-practices)
10. [AI Agent Development Workflow](#ai-agent-development-workflow)
11. [Frontend Component Development Guidelines](#frontend-component-development-guidelines)
12. [Metrics Reporting and Monitoring](#metrics-reporting-and-monitoring)
13. [drf_resource Module Usage Guide](#drf_resource-module-usage-guide)
14. [Custom Report Model Design Specification](#custom-report-model-design-specification)
15. [Conclusion](#conclusion)

## Introduction
BlueKing Monitor Platform is an open-source monitoring system developed by the Tencent BlueKing team, designed to provide comprehensive data collection, large-scale data processing capabilities, and flexible platform extensibility. This development guide aims to provide contributors with complete instructions for setting up the development environment, code contribution processes, toolchain usage, and best practices to ensure code quality and project consistency.

The platform leverages BlueKing PaaS to support various monitoring scenarios, striving to enhance the timeliness, accuracy, and intelligence of monitoring to safeguard online services. Developers can quickly get started and participate in project contributions through this guide.

**Section sources**
- [README.md](file://README.md#L1-L51)

## Project Structure
The project adopts a modular design with the following main directories:

- `ai_agent`: Core logic and services for AI agents
- `bkmonitor`: Main monitoring application containing core modules such as alerts, data sources, and strategies
- `api`: Encapsulation of various external system interfaces
- `config`: Configuration management separated by environment and role
- `scripts`: Script tools including pre-commit hooks
- `wiki`: Internal knowledge documentation
- `tests`: Test code
- `bin`: Executable scripts

The core application `bkmonitor` contains multiple sub-modules such as `alarm_backends` (alert backend), `apm` (application performance monitoring), `data_source` (data source management), etc., reflecting a clear functional division.

```
mermaid
graph TD
A[Project Root Directory] --> B[ai_agent]
A --> C[bkmonitor]
A --> D[api]
A --> E[config]
A --> F[scripts]
A --> G[wiki]
C --> H[alarm_backends]
C --> I[apm]
C --> J[data_source]
C --> K[strategy]
C --> L[views]
C --> M[models]
F --> N[pre-commit]
N --> O[check_commit_message.py]
N --> P[pre-push.py]
```

**Diagram sources**
- [README.md](file://README.md#L1-L51)

## Code Style and Conventions
The project follows unified code style conventions to ensure code readability and consistency. The main conventions are:

- **Python Code Formatting**: Use the `black` tool for code formatting with a line length limit of 120 characters.
- **Import Sorting**: Use `isort` to sort import statements following the `black` style.
- **Static Checking**: Use `flake8` and `ruff` for code quality checks, ignoring specific rules (such as E203, W503) to be compatible with `black`.
- **Naming Conventions**: Follow PEP8 naming conventions, using lowercase with underscores for variables and functions, and camel case for class names.

These rules are explicitly defined in the `pyproject.toml` file. Developers should configure their editor plugins to automatically execute formatting and checks.

```toml
[tool.black]
line-length = 120
include = '\.pyi?$'
skip-string-normalization = true

[tool.isort]
profile = "black"
src_paths = ["bkmonitor", "bklog", "bkmonitor/packages"]

[tool.ruff]
src = ["bkmonitor", "bklog", "bkmonitor/packages"]
line-length = 120
target-version = "py310"
```

**Section sources**   
- [pyproject.toml](file://pyproject.toml#L1-L63)

## Dependency Management and Build Configuration
The project uses `pyproject.toml` as the standard configuration file for modern Python projects, replacing traditional `setup.py` or `requirements.txt`. This file not only defines code style tools (such as black, isort, ruff) but also specifies project dependencies and build metadata.

Dependency management features:
- All third-party packages are declared in `known_third_party` for proper import classification by `isort`.
- Uses `src` source layout with main code located in the `bkmonitor/` directory.
- Supports dynamic override of configuration items through environment variables prefixed with `BKAPP_SETTINGS_`.

The build process is managed through `Makefile` and scripts in the `bin/` directory (such as `manage.sh`, `release.sh`), supporting different requirements for local development, testing, and production deployment.

**Section sources**   
- [pyproject.toml](file://pyproject.toml#L1-L63)

## Pre-commit Hooks and Code Checks
The project implements strict pre-commit checks through scripts in the `scripts/pre-commit/` directory to ensure code quality and commit message standards.

### Commit Message Standard Check
The `check_commit_message.py` script enforces that all Git commit messages must contain specific prefixes, following the Conventional Commits specification:

```python
ALLOWED_COMMIT_MSG_PREFIX = [
    ("feat", "A new feature. Correlates with MINOR in SemVer"),
    ("fix", "A bug fix. Correlates with PATCH in SemVer"),
    ("docs", "Documentation only changes"),
    ("style", "Changes that do not affect the meaning of the code"),
    ("refactor", "A code change that neither fixes a bug nor adds a feature"),
    ("perf", "A code change that improves performance"),
    ("test", "Adding missing or correcting existing tests"),
    ("chore", "Changes to the build process or auxiliary tools and libraries such as documentation generation"),
    ("merge", "Merge branch and fix conflicts"),
]
```

Commit messages must match the regular expression `^(\w+)(\(\S+\))?!?:\s*([^\n\r]+)$`, for example: `feat(api): add new endpoint`.

### Pre-push Automation Check
The `pre-push.py` script executes automated checks before `git push`:
1. Detects the existence of the `preci` tool
2. Retrieves the list of changed files between local and remote branches
3. Copies changed files to a temporary directory
4. Calls `preci run` to execute pre-integration checks (PreCI)
5. Determines whether to allow pushing based on check results

This process ensures that only code passing automated checks can be pushed to the remote repository.

```
mermaid
sequenceDiagram
participant Developer as Developer
participant Git as Git
participant PreCommit as pre-push.py
participant PreCI as PreCI System
Developer->>Git : git push origin feature/x
Git->>PreCommit : Trigger pre-push hook
PreCommit->>PreCommit : Get list of changed files
PreCommit->>PreCommit : Copy files to temporary directory
PreCommit->>PreCI : Execute preci run --projectPath
PreCI-->>PreCommit : Return check results
alt Check Passed
PreCommit-->>Git : Allow push
Git-->>Developer : Push successful
else Check Failed
PreCommit-->>Developer : Block push and report error
end
```

**Diagram sources**
- [pre-push.py](file://scripts/pre-commit/pre-push.py#L1-L182)
- [check_commit_message.py](file://scripts/pre-commit/check_commit_message.py#L1-L61)

**Section sources**
- [check_commit_message.py](file://scripts/pre-commit/check_commit_message.py#L1-L61)
- [pre-push.py](file://scripts/pre-commit/pre-push.py#L1-L182)
- [preci.py](file://scripts/pre-commit/preci.py#L1-L25)
- [preci.sh](file://scripts/pre-commit/preci.sh#L1-L20)

## Configuration Management
The project adopts a layered configuration management approach, dynamically loading configurations through `settings.py`:

1. **Base Configuration**: `config/default.py`
2. **Environment Configuration**: `config/{dev|stag|prod}.py`, loaded according to the `ENVIRONMENT` variable
3. **Role Configuration**: `config/role/{api|web|worker}.py`, loaded according to the `ROLE` variable
4. **Environment Variable Override**: Environment variables prefixed with `BKAPP_SETTINGS_` can dynamically override configurations

The configuration loading order is: `config.default` → `blueapps.patch` → `config.{env}` → `config.role.{role}`.

In development mode, local configuration can be overridden through the `local_settings.py` file, which is ignored by `.gitignore` to prevent sensitive information leakage.

```python
DJANGO_CONF_MODULE = "config.{env}".format(
    env={"development": "dev", "testing": "stag", "production": "prod"}.get(ENVIRONMENT)
)

# Load role configuration
try:
    _module = __import__(f"config.role.{ROLE}", globals(), locals(), ["*"])
except ImportError as e:
    logging.exception(e)
    raise ImportError("Could not import config '{}' (Is it on sys.path?): {}".format(f"config.role.{ROLE}", e))
```

**Section sources**
- [settings.py](file://bkmonitor/settings.py#L1-L73)

## Testing Strategy
The project employs a multi-layered testing strategy to ensure code quality:

- **Unit Testing**: Uses the `pytest` framework with test cases located in the `tests/` directory of each module.
- **Integration Testing**: Implemented in directories such as `alarm_backends/integration_tests/` for cross-module integration testing.
- **Automated Testing**: Configured through `.github/workflows/pytest.yml` to automatically run tests on every PR via GitHub Actions.

Testing covers core functionalities such as alert processing, strategy calculation, and data collection. Developers must provide corresponding test cases when submitting new features or bug fixes.

**Section sources**
- [pytest.yml](file://bkmonitor/.github/workflows/pytest.yml#L1-L69)

## CI/CD Process
The project's CI/CD process consists of two main components:

1. **Local Pre-commit Checks**: Through `pre-commit` hooks that check code style and commit messages before committing.
2. **Pre-push Pre-integration Checks**: Through the `pre-push.py` script that calls the `preci` system to execute automated tests and code analysis.
3. **GitHub Actions**: Configured in `.github/workflows/pytest.yml` to run automated tests on every push and pull request, ensuring code quality throughout the development lifecycle.

The complete workflow ensures quality assurance at every stage from local development to code merging.

``mermaid
graph LR
A[Local Development] --> B[Pre-commit Checks]
B --> C[Code Commit]
C --> D[Pre-push PreCI]
D --> E[Push to Remote]
E --> F[GitHub Actions CI]
F --> G[Pull Request]
G --> H[Code Review]
H --> I[Merge to Main]
```

**Diagram sources**
- [pytest.yml](file://bkmonitor/.github/workflows/pytest.yml#L1-L69)

## Contribution Process and Best Practices
### Contribution Path
1. Fork the repository and clone it locally
2. Create a feature branch `git checkout -b feature/your-feature`
3. Write code ensuring compliance with code style
4. Commit with standardized commit messages, e.g., `feat: add new feature`
5. Push code, triggering `pre-push` checks
6. Create a Pull Request on GitHub
7. Participate in code review and make modifications based on feedback
8. Merge into the main branch

### Best Practices
- **Code Review**: All PRs must be reviewed by at least one core team member.
- **Documentation Updates**: New features require synchronized updates to relevant documentation.
- **Backward Compatibility**: Avoid breaking changes; major changes require prior discussion.
- **Performance Considerations**: Conduct benchmark testing for performance-sensitive code.
- **Error Handling**: Implement comprehensive exception handling with clear error messages.
- **AI Agent Development**: When developing AI agent features, ensure proper metrics reporting and follow the established command handling patterns.

New developers should first read `docs/CONTRIBUTING.md` and `README.md` to understand the project background and contribution guidelines.

**Section sources**
- [README.md](file://README.md#L1-L51)

## AI Agent Development Workflow
The AI agent component provides a comprehensive SDK for integrating AI capabilities into the monitoring platform. This section details the development workflow and best practices for AI agent development.

### Core Architecture
The AI agent system consists of three main components:
1. **AIDevInterface**: The core interface class that orchestrates AI operations
2. **LocalCommandProcessor**: Handles local command processing and rendering
3. **AIMetricsReporter**: Manages metrics collection and reporting for AI operations

### Development Workflow
1. **Initialize the Interface**: Create an instance of `AIDevInterface` with app credentials and metrics reporter
2. **Implement Command Handlers**: Use the `@local_command_handler` decorator to register custom command processors
3. **Handle Session Management**: Implement session creation, retrieval, and destruction through the interface methods
4. **Process Chat Completion**: Implement both streaming and non-streaming chat completion with proper metrics reporting

### Key Implementation Patterns
- **Command Registration**: Use the decorator pattern to register local command handlers
- **Metrics Integration**: Ensure all AI resources are decorated with metrics reporting
- **Error Handling**: Implement comprehensive error handling with proper logging
- **Streaming Support**: Use the enhanced streaming response wrapper for real-time AI interactions

```python
@local_command_handler("tracing_analysis")
class TracingAnalysisCommandHandler(CommandHandler):
    def process_content(self, context: list[dict]) -> str:
        # Implement processing logic
        pass
```

**Section sources**
- [aidev_interface.py](file://ai_agent/core/aidev_interface.py#L1-L215)
- [local_command_handler.py](file://ai_agent/services/local_command_handler.py#L1-L119)
- [metrics_reporter.py](file://ai_agent/services/metrics_reporter.py#L1-L418)

## Frontend Component Development Guidelines
This section provides guidelines for developing and maintaining frontend components, particularly focusing on the custom metrics view components.

### Custom Metrics View Implementation
The custom metrics view components are responsible for displaying time-series data with customizable refresh intervals. Key considerations include:

- **Refresh Interval Management**: Implement proper refresh interval logic to balance data freshness and performance
- **State Management**: Maintain component state efficiently to prevent unnecessary re-renders
- **Error Handling**: Implement robust error handling for data loading failures

### Refresh Interval Fix
A recent bug fix addressed the custom metrics view refresh interval issue by implementing a timeout-based refresh mechanism:

```python
def should_refresh_metric_json(self, timeout=5 * 60):
    """
    Determine if the current metric json needs refresh, default refresh time is 5 minutes
    to avoid synchronizing data every time the page is opened.
    :param timeout: Expiration time in seconds
    :return: True if refresh is needed, False otherwise
    """
    update_time = arrow.get(self.current_version.info.update_time)
    current_time = arrow.now(tz=update_time.tzinfo)
    time_delta = current_time.timestamp - update_time.timestamp
    if time_delta > timeout:
        return True
    return False
```

This implementation prevents excessive data synchronization by only refreshing when the data has expired beyond the specified timeout.

**Section sources**
- [plugin.py](file://bkmonitor/packages/monitor_web/models/plugin.py#L510-L540)

## Metrics Reporting and Monitoring
The AI agent system includes a comprehensive metrics reporting infrastructure to monitor performance and usage patterns.

### Metrics Reporter Implementation
The `AIMetricsReporter` class provides the foundation for collecting and reporting AI service metrics:

- **Request Tracking**: Tracks request count and duration for each resource
- **Status Monitoring**: Categorizes requests by status (success, error, streaming, etc.)
- **Label-Based Aggregation**: Supports aggregation by agent code, resource name, username, and command

### Decorator-Based Metrics Collection
The system uses decorators to automatically collect metrics for AI resources:

```python
@ai_metrics_decorator(ai_metrics_reporter=metrics_reporter)
def perform_request(self, validated_request_data):
    # Business logic here
    pass
```

For streaming responses, an enhanced decorator provides detailed metrics about chunk count, transmission time, and data size.

### Streaming Response Metrics
The enhanced streaming metrics system tracks:
- Setup duration (time to first chunk)
- Total streaming duration
- Number of data chunks
- Total data size
- Error conditions during streaming

This comprehensive metrics system enables detailed performance analysis and monitoring of AI agent interactions.

**Section sources**
- [metrics_reporter.py](file://ai_agent/services/metrics_reporter.py#L1-L418)
- [utils.py](file://ai_agent/utils.py#L1-L81)

## drf_resource Module Usage Guide
The `drf_resource` module is a core framework in the project used for building API interfaces. It abstracts view logic into `Resource` classes, greatly simplifying the development process.

### Core Concepts
- **Resource**: Represents an independent business logic unit and is the basic building block of APIs. Each `Resource` class contains a `perform_request` method for implementing specific business logic.
- **Serializer**: Used for request data validation and response data serialization. `RequestSerializer` validates input, while `ResponseSerializer` defines output format.
- **ResourceViewSet**: Used to organize and route `Resource` classes, mapping them to specific URLs through `ResourceRoute` configuration.

### Usage Method
1. **Create Resource**: Define a class inheriting from `Resource` in the `resources.py` file and implement the `perform_request` method.
2. **Define Serializer**: Declare `RequestSerializer` and `ResponseSerializer` for the `Resource` to ensure data correctness and consistency.
3. **Configure Routing**: Create a `ResourceViewSet` in `views.py` and register the `Resource` to a specific URL path through the `resource_routes` list.
4. **Register Routing**: Use the `register_module` method of `ResourceRouter` in `urls.py` to automatically scan and register all `ViewSet` classes.

### Invocation Methods
`Resource` classes can be invoked in two ways:
- **Instance Invocation**: Create a `Resource` instance and call its `request` method.
- **Unified Entry Invocation**: Call through the attribute chain of the `resource` object, for example, `resource.app0.update_user_info()`.

```python
from bk_resource import resource

# Call through unified entry
result = resource.app0.update_user_info(new_username="BlueKing")
```

**Section sources**
- [drf_resource模块的使用.md](file://wiki/drf_resource模块的使用.md#L1-L480)

## Custom Report Model Design Specification
The `TimeSeriesGroup` model is the core of the custom reporting functionality, with a complex and sophisticated design aimed at supporting flexible time-series data storage and querying.

### Core Design Intentions
The model's design aims to solve the following problems:
1. **Multi-tenancy Support**: Achieves multi-tenancy mode result table ID generation through the `bk_tenant_id` field and `make_table_id` method.
2. **Flexible Storage Configuration**: Allows different grouping configurations with different storage strategies through `DEFAULT_STORAGE_CONFIG` and `process_default_storage_config` methods.
3. **Efficient Metadata Management**: Implements batch creation, updating, and deduplication of metrics and dimensions through methods like `update_metrics` and `bulk_refresh_rt_fields`, ensuring metadata accuracy and consistency.
4. **External System Integration**: Achieves seamless integration with external systems like `transfer` and `bkdata` through methods like `get_metrics_from_redis` and `get_metric_from_bkdata`, ensuring real-time synchronization of metric data.

### Key Method Analysis
- **`make_table_id`**: Dynamically generates a standardized result table ID based on the multi-tenancy switch and business ID.
- **`update_metrics`**: Coordinates the `TimeSeriesMetric` and `ResultTableField` models to ensure consistency of metric information across different levels.
- **`get_metrics_from_redis`**: Efficiently pulls metric and dimension data from Redis, key to implementing auto-discovery functionality.
- **`to_json_v2` / `to_split_json`**: Provides JSON serialization methods compatible with new and old versions, supporting both single-table and sharded table storage modes.

Understanding these design and implementation details is crucial for maintaining and extending the custom reporting functionality.

**Section sources**
- [time_series.py](file://bkmonitor/metadata/models/custom_report/time_series.py#L1-L1485)
- [base.py](file://bkmonitor/metadata/models/custom_report/base.py#L1-L523)

## Conclusion
This development guide provides a detailed overview of the BlueKing Monitor platform's development environment setup, coding standards, toolchain usage, testing strategies, and contribution processes. By following these guidelines and best practices, contributors can efficiently participate in project development, ensuring code quality and project stability. It is recommended that all new contributors read this guide thoroughly before starting to code and refer to it continuously during development.