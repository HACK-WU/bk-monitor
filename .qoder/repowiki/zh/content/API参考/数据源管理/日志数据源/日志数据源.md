# 日志数据源

<cite>
**本文档引用的文件**  
- [storage.py](file://bkmonitor/metadata/models/storage.py)
- [metadata_create_cluster_info.md](file://bkmonitor/docs/api/apidocs/zh_hans/metadata_create_cluster_info.md)
- [metadata_modify_cluster_info.md](file://bkmonitor/docs/api/apidocs/zh_hans/metadata_modify_cluster_info.md)
- [metadata_create_result_table.md](file://bkmonitor/support-files/apigw/docs/zh/metadata_create_result_table.md)
- [event_related_info.py](file://bkmonitor/bkmonitor/utils/event_related_info.py)
- [data_source/__init__.py](file://bkmonitor/bkmonitor/data_source/data_source/__init__.py)
- [test_compose_data_link_configs.py](file://bkmonitor/metadata/tests/data_link/test_compose_data_link_configs.py)
- [test_es_resources.py](file://bkmonitor/metadata/tests/resources/test_es_resources.py)
- [alert_log.py](file://bkmonitor/packages/fta_web/alert/handlers/alert_log.py)
- [grafana_log_query.md](file://bkmonitor/support-files/apigw/docs/zh/grafana_log_query.md)
- [v1.py](file://bkmonitor/packages/monitor_web/strategies/resources/v1.py)
- [sync_metadata.py](file://bkmonitor/metadata/service/sync_metadata.py)
- [diagnostic.md](file://bkmonitor/metadata/agents/prompts/diagnostic.md)
</cite>

## 目录
1. [简介](#简介)
2. [日志系统接入方式](#日志系统接入方式)
3. [日志解析机制](#日志解析机制)
4. [日志字段映射与转换](#日志字段映射与转换)
5. [日志级别处理](#日志级别处理)
6. [日志查询语言](#日志查询语言)
7. [配置与查询示例](#配置与查询示例)
8. [存储策略与性能优化](#存储策略与性能优化)
9. [系统集成](#系统集成)
10. [附录](#附录)

## 简介

本文档详细介绍了日志数据源的接入、解析、查询、存储及集成机制。系统支持多种日志数据源的接入，包括蓝鲸日志平台（BK_LOG_SEARCH）、自研日志系统等。日志数据通过统一的查询接口进行检索，支持全文检索、字段过滤和聚合分析。系统提供灵活的字段映射和转换规则，支持正则表达式、JSON和分隔符等多种解析方式。日志数据存储在Elasticsearch、InfluxDB等集群中，支持按时间、大小进行索引轮转和生命周期管理。日志系统与告警引擎、分析平台深度集成，为运维监控提供全面支持。

## 日志系统接入方式

日志数据源的接入主要通过元数据管理模块进行配置和管理。系统支持多种存储类型，包括Elasticsearch、InfluxDB、Kafka、Redis等。接入流程包括创建存储集群配置和结果表配置两个主要步骤。

### 存储集群配置

存储集群配置定义了日志数据的存储位置和连接信息。通过`metadata_create_cluster_info`接口创建存储集群，主要参数包括：

| 字段 | 类型 | 必选 | 描述 |
| ---- | ---- | ---- | ---- |
| cluster_name | string | 是 | 存储集群名 |
| cluster_type | string | 是 | 存储集群类型，支持influxDB、kafka、redis、elasticsearch |
| domain_name | string | 是 | 存储集群域名（可填入IP） |
| port | int | 是 | 存储集群端口 |
| operator | string | 是 | 创建者 |
| description | string | 否 | 存储集群描述信息 |
| auth_info | dict | 否 | 集群身份验证信息 |
| version | string | 否 | 集群版本信息 |

```json
{
    "bk_app_code": "xxx",
    "bk_app_secret": "xxxxx",
    "bk_token": "xxxx",
    "cluster_name": "first_influxdb",
    "cluster_type": "influxDB",
    "domain_name": "influxdb.service.consul",
    "operator": "admin",
    "auth_info": {
        "username": "username",
        "password": "password"
    },
    "port": 9052,
    "description": "描述信息"
}
```

### 结果表配置

结果表配置定义了日志数据的存储结构和策略。通过`metadata_create_result_table`接口创建结果表，其中存储配置参数根据存储类型有所不同。

**Elasticsearch存储参数：**
| 键值 | 类型 | 是否必选 | 默认值 | 描述 |
| ---- | ---- | ---- | ---- | ---- |
| storage_cluster_id | int | 否 | 使用该存储类型的默认存储集群 | 指定存储集群 |
| retention | int | 否 | 30 | 保留index时间，单位为天 |
| date_format | string | 否 | %Y%m%d%H | 时间格式，默认具体到小时 |
| slice_size | int | 否 | 500 | 需要切分的大小阈值，单位为GB |
| slice_gap | int | 否 | 120 | index分片时间间隔，单位分钟 |
| index_settings | string | 是 | - | 索引创建配置，json格式 |
| mapping_settings | string | 否 | - | 索引mapping配置，不包含字段定义 |

**实际index构造方式为`${table_id}_${date_format}_${current_index}`**

**Redis存储参数：**
| 键值 | 类型 | 是否必选 | 默认值 | 描述 |
| ---- | ---- | ---- | ---- | ---- |
| storage_cluster_id | int | 否 | 使用该存储类型的默认存储集群 | 指定存储集群 |
| key | string | 否 | table_id名字 | 存储键值 |
| db | int | 否 | 0 | 使用db配置 |
| command | string | 否 | PUBLISH | 存储命令 |
| is_sentinel | bool | 否 | False | 是否哨兵模式 |
| master_name | string | 否 | "" | 哨兵模式下master名称 |

**注意**：由于redis默认使用队列方式，消费后就丢弃，因此未有时长配置。

**Section sources**
- [metadata_create_cluster_info.md](file://bkmonitor/docs/api/apidocs/zh_hans/metadata_create_cluster_info.md)
- [metadata_create_result_table.md](file://bkmonitor/support-files/apigw/docs/zh/metadata_create_result_table.md)
- [storage.py](file://bkmonitor/metadata/models/storage.py)

## 日志解析机制

日志解析机制支持多种格式的识别和解析，包括正则表达式提取、JSON解析和分隔符解析。系统通过配置化的解析规则，将原始日志转换为结构化的数据。

### 日志格式识别

日志格式识别主要通过采集配置中的字段定义来实现。在`LogCollectorConfig`模型中，`sep`字段定义了数据分隔符，`fields`字段定义了字段配置。

```python
class LogCollectorConfig(OperateRecordModel):
    """
    日志接入配置
    """
    biz_id = models.IntegerField("业务id")
    data_id = models.CharField("数据源ID", max_length=100, default="")
    result_table_id = models.CharField("结果表ID", max_length=100, default="")
    data_set = models.CharField("数据源表名", max_length=100)
    data_desc = models.CharField("数据源中文名", max_length=100)
    data_encode = models.CharField("字符编码", max_length=30)
    sep = models.CharField("数据分隔符", max_length=30)
    log_path = models.TextField("日志路径")
    fields = JsonField("字段配置")
    ips = JsonField("采集对象ip列表")
    conditions = JsonField("采集条件")
    file_frequency = models.CharField("日志生成频率", max_length=30)
```

### 解析方法实现

系统支持多种解析方法，通过配置选择合适的解析器。对于分隔符格式的日志，使用分隔符解析；对于JSON格式的日志，使用JSON解析；对于复杂格式的日志，使用正则表达式提取。

在`event_related_info.py`中，`get_event_related_info`函数实现了日志查询和解析的核心逻辑：

```python
def get_event_related_info(alert, is_raw=False):
    """
    获取告警关联日志信息的主函数，包含重试机制
    
    参数:
        alert: 告警文档对象，包含策略配置和事件信息
        is_raw: 布尔值，指示是否返回原始日志数据（默认False）
    
    返回值:
        成功时返回日志内容（字符串或原始数据结构）
        失败时返回空值或抛出异常（当两次尝试均失败时）
    
    该函数实现以下核心流程：
    1. 加载并初始化数据源实例
    2. 构建维度过滤条件
    3. 首次尝试获取日志内容
    4. 异常处理及重试机制
    """
    # 加载数据源类并初始化实例
    query_config = alert.strategy["items"][0]["query_configs"][0]
    data_source_class = load_data_source(query_config["data_source_label"], query_config["data_type_label"])
    data_source = data_source_class.init_by_query_config(query_config, bk_biz_id=alert.event.bk_biz_id)
    
    # 构建维度过滤条件：仅保留聚合维度相关的字段
    data_source.filter_dict.update(
        {
            key: value
            for key, value in alert.origin_alarm.get("data", {}).get("dimensions", {}).items()
            if key in query_config.get("agg_dimension", [])
        }
    )
    
    # 配置重试间隔时间（毫秒转秒）
    retry_interval = settings.DELAY_TO_GET_RELATED_INFO_INTERVAL
```

**Section sources**
- [event_related_info.py](file://bkmonitor/bkmonitor/utils/event_related_info.py)
- [old.py](file://bkmonitor/packages/monitor/models/old.py)

## 日志字段映射与转换

日志字段映射与转换机制允许用户自定义字段别名和转换规则，将原始日志字段映射到标准的字段模型。

### 字段别名映射

系统支持字段别名映射，通过`ESFieldQueryAliasOption`模型存储字段别名配置。`_get_field_alias_map`函数实现了字段别名映射的获取逻辑：

```python
def _get_field_alias_map(table_id_list, bk_tenant_id):
    """
    获取字段别名映射map
    
    @param table_id_list: 结果表ID列表
    @param bk_tenant_id: 租户ID
    @return: 字段别名映射map
    """
    if not table_id_list:
        return {}
    try:
        # 获取指定table_id列表的未删除别名记录
        alias_records = models.ESFieldQueryAliasOption.objects.filter(
            table_id__in=table_id_list, is_deleted=False, bk_tenant_id=bk_tenant_id
        ).values("table_id", "query_alias", "field_path")

        # 按table_id分组构建别名映射
        field_alias_map = {}
        for record in alias_records:
            table_id = record["table_id"]
            query_alias = record["query_alias"]
            field_path = record["field_path"]

            if table_id not in field_alias_map:
                field_alias_map[table_id] = {}

            field_alias_map[table_id][query_alias] = field_path

        return field_alias_map
    except Exception as e:
        logger.error("_get_field_alias_map:Error getting field alias map for table_ids: %s, error: %s", table_id_list, e)
        return {}
```

### 字段配置示例

在测试用例中，定义了详细的字段配置，包括字段别名、类型、描述等信息：

```python
fields = [
    {
        "field_alias": "",
        "field_index": 0,
        "field_name": "dimensions",
        "field_type": "object",
        "is_dimension": True,
    },
    {
        "field_alias": "",
        "field_index": 1,
        "field_name": "event",
        "field_type": "object",
        "is_dimension": True,
    },
    {
        "field_alias": "",
        "field_index": 2,
        "field_name": "event_name",
        "field_type": "string",
        "is_dimension": True,
    },
    {
        "field_alias": "",
        "field_index": 3,
        "field_name": "target",
        "field_type": "string",
        "is_dimension": True,
    },
    {
        "field_alias": "数据上报时间",
        "field_index": 4,
        "field_name": "time",
        "field_type": "timestamp",
        "is_dimension": False,
    },
]
```

**Section sources**
- [space_table_id_redis.py](file://bkmonitor/metadata/models/space/space_table_id_redis.py)
- [test_compose_data_link_configs.py](file://bkmonitor/metadata/tests/data_link/test_compose_data_link_configs.py)

## 日志级别处理

日志级别处理机制通过告警日志处理器实现，将不同类型的告警事件转换为相应的日志记录。

### 告警日志处理器

`AlertLogHandler`类负责处理各种告警事件，将其转换为日志记录。不同的告警操作类型对应不同的日志记录方法：

```python
class AlertLogHandler:
    def add_record_alert(self, hit, record):
        """告警产生"""
        contents = [hit.description]
        source_time = utc2datetime(self.alert.begin_time)

        if self.alert.strategy and self.alert.strategy["items"][0]["algorithms"]:
            item = self.alert.strategy["items"][0]
            detect = self.alert.strategy["detects"][0]
            if NO_DATA_TAG_DIMENSION in self.alert.origin_alarm["data"]["dimensions"]:
                continuous = item["no_data_config"]["continuous"]
                contents.append(_(" 达到了触发告警条件（数据连续丢失{}个周期）").format(continuous))
            else:
                trigger_config = detect["trigger_config"]
                contents.append(
                    _(" 达到了触发告警条件（{}周期内满足{}次检测算法）").format(trigger_config["check_window"], trigger_config["count"])
                )
        record.update({"source_time": source_time, "index": 0, "contents": contents})
        self.log_records.append(record)

    def add_record_recover(self, hit, record):
        """告警恢复"""
        record.update({"contents": [hit.description]})
        self.log_records.append(record)

    def add_record_delay_recover(self, hit, record):
        """延时恢复"""
        record.update(
            {"contents": [hit.description, _("根据系统配置，告警将于 {} 延时恢复").format(utc2datetime(hit.next_status_time))]}
        )
        self.log_records.append(record)

    def add_record_abort_recover(self, hit, record):
        """中断恢复"""
        if hit.description:
            contents = [_("{}确认了该告警事件并备注：").format(hit.operator), hit.description]
        else:
            contents = [_("在延时恢复的时间窗口收到了新的异常事件，延时恢复被中断")]
        record.update({"contents": contents})
        self.log_records.append(record)

    def add_record_system_recover(self, hit, record):
        """系统恢复"""
        record.update({"contents": [_("延时恢复时间窗口结束，告警已恢复")]})
        self.log_records.append(record)

    def add_record_ack(self, hit, record):
        """确认告警"""
        if hit.description:
            contents = [_("{}确认了该告警事件并备注：").format(hit.operator), hit.description]
        else:
            contents = [_("{}确认了该告警事件").format(hit.operator)]
        record.update({"contents": contents})
        self.log_records.append(record)

    def add_record_close(self, hit, record):
        """关闭告警"""
        record.update({"contents": [hit.description]})
        self.log_records.append(record)

    def add_record_event_drop(self, hit, record):
        """事件丢弃"""
        contents = [hit.description, _("告警级别【{}】低于当前告警触发级别，系统已忽略").format(EventSeverity.get_display_name(hit.severity))]
        self.record_collect(hit, record, AlertLog.OpType.EVENT_DROP, contents)
```

**Section sources**
- [alert_log.py](file://bkmonitor/packages/fta_web/alert/handlers/alert_log.py)

## 日志查询语言

日志查询语言支持全文检索、字段过滤和聚合分析等功能，通过统一的查询接口实现。

### 查询语法

日志查询接口支持多种查询参数，包括全文检索、字段过滤、时间范围等。

**接口参数：**
| 字段 | 类型 | 必选 | 描述 |
| ---- | ---- | ---- | ---- |
| data_format | str | 否 | 数据格式 |
| bk_biz_id | int | 是 | 业务ID |
| data_source_label | str | 是 | 数据来源 |
| data_type_label | str | 是 | 数据类型 |
| query_string | str | 否 | 查询字符串 |
| index_set_id | str | 否 | 索引集ID |
| alert_name | str | 否 | 告警名称 |
| bkmonitor_strategy_id | str | 否 | 策略ID |
| result_table_id | str | 否 | 结果表ID |
| where | list | 否 | 过滤条件 |
| filter_dict | dict | 否 | 过滤字典 |
| start_time | int | 否 | 开始时间 |
| end_time | int | 否 | 结束时间 |
| limit | int | 否 | 查询条数 |
| offset | int | 否 | 查询偏移 |

**where条件：**
| 字段 | 类型 | 必选 | 描述 |
| ---- | ---- | ---- | ---- |
| key | str | 是 | 条件的key |
| method | str | 是 | 方法 |
| value | list[str] | 是 | 条件的value |

### 查询实现

`query_log`方法实现了日志查询的核心逻辑，支持时间范围、分页和限制返回数量：

```python
def query_log(
    self,
    start_time: int = None,
    end_time: int = None,
    limit: int | None = None,
    offset: int | None = None,
    *args,
    **kwargs,
) -> tuple[list, int]:
    """
    查询原始日志数据

    参数:
        start_time: 查询起始时间戳（毫秒）
        end_time: 查询结束时间戳（毫秒）
        limit: 返回日志的最大数量
        offset: 分页偏移量
        *args: 其他位置参数（传递给父类）
        **kwargs: 其他关键字参数（传递给父类）

    返回值:
        二元组包含：
        - 日志数据列表（包含_source字段的原始日志）
        - 符合条件的总日志数量

    查询流程：
    1. 构建日志查询对象
    2. 解析返回数据结构
    3. 提取日志内容和总数
    4. 处理total字段的可能嵌套结构
    """
```

**Section sources**
- [grafana_log_query.md](file://bkmonitor/support-files/apigw/docs/zh/grafana_log_query.md)
- [__init__.py](file://bkmonitor/bkmonitor/data_source/data_source/__init__.py)

## 配置与查询示例

### 配置示例

**创建Elasticsearch存储集群：**
```json
{
    "bk_app_code": "xxx",
    "bk_app_secret": "xxxxx",
    "bk_token": "xxxx",
    "cluster_name": "es_cluster_01",
    "cluster_type": "elasticsearch",
    "domain_name": "es.service.consul",
    "port": 9200,
    "operator": "admin",
    "auth_info": {
        "username": "admin",
        "password": "password"
    },
    "description": "生产环境Elasticsearch集群"
}
```

**创建结果表配置：**
```json
{
    "bk_app_code": "xxx",
    "bk_app_secret": "xxxxx",
    "bk_token": "xxxx",
    "table_id": "7_bklog.app_log",
    "table_name_zh": "应用日志",
    "is_custom_table": true,
    "schema_type": "free",
    "default_storage": "elasticsearch",
    "storage_config": {
        "storage_cluster_id": 1,
        "retention": 7,
        "date_format": "%Y%m%d",
        "slice_size": 100,
        "slice_gap": 60,
        "index_settings": "{\"number_of_shards\": 3, \"number_of_replicas\": 1}",
        "mapping_settings": "{}"
    },
    "field_list": [
        {
            "field_name": "log",
            "field_type": "string",
            "description": "日志内容",
            "tag": "metric",
            "option": {}
        },
        {
            "field_name": "time",
            "field_type": "timestamp",
            "description": "日志时间",
            "tag": "dimension",
            "option": {
                "es_type": "date",
                "es_format": "epoch_millis"
            }
        }
    ]
}
```

### 查询示例

**查询应用日志：**
```json
{
    "data_source_label": "bk_log_search",
    "data_type_label": "log",
    "bk_biz_id": 2,
    "index_set_id": 53,
    "result_table_id": "7_bklog.cmdb_prod",
    "query_string": "\"MUST_SEND_ALARM\"",
    "where": [
        {
            "key": "path",
            "method": "eq",
            "value": [
                "/data/xxx/logs/cmdb/xx.cmdb-sync-1.xxxx.log.INFO.20250220-132529.30676"
            ]
        },
        {
            "condition": "and",
            "key": "serverIp",
            "method": "eq",
            "value": [
                "127.0.0.1"
            ]
        }
    ],
    "start_time": 1740028800,
    "end_time": 1740034724,
    "limit": 20,
    "offset": 0
}
```

**响应结果：**
```json
{
    "result": true,
    "code": 200,
    "message": "OK",
    "data": {
        "log_detail": ""
    }
}
```

**Section sources**
- [metadata_create_cluster_info.md](file://bkmonitor/docs/api/apidocs/zh_hans/metadata_create_cluster_info.md)
- [metadata_create_result_table.md](file://bkmonitor/support-files/apigw/docs/zh/metadata_create_result_table.md)
- [grafana_log_query.md](file://bkmonitor/support-files/apigw/docs/zh/grafana_log_query.md)

## 存储策略与性能优化

### 存储策略

日志数据存储策略通过元数据管理模块进行配置，支持多种存储类型和生命周期管理。

**Elasticsearch存储策略：**
- **保留时间**：通过`retention`参数配置，单位为天，默认30天
- **索引切分**：通过`slice_size`和`slice_gap`参数配置，按大小或时间切分索引
- **索引设置**：通过`index_settings`和`mapping_settings`参数配置索引的分片和副本数量

**InfluxDB存储策略：**
- **代理存储**：通过`InfluxDBProxyStorage`模型配置代理存储，支持高可用和负载均衡
- **集群配置**：支持多集群配置，通过`proxy_cluster_id`和`instance_cluster_name`进行匹配

```python
class InfluxDBProxyStorage(models.Model):
    """
    InfluxDB代理存储配置
    """
    proxy_cluster_id = models.IntegerField("代理集群ID")
    instance_cluster_name = models.CharField("实例集群名称", max_length=128)
    is_default = models.BooleanField("是否默认", default=False)
    description = models.TextField("描述", null=True, blank=True)
```

### 性能优化

系统通过多种机制优化日志查询性能：

1. **索引优化**：合理配置Elasticsearch索引的分片和副本数量
2. **查询缓存**：对常用查询结果进行缓存
3. **字段裁剪**：只查询必要的字段，减少数据传输量
4. **分页查询**：通过limit和offset参数实现分页，避免一次性返回大量数据
5. **并行查询**：对多个索引集的查询使用线程池并行执行

```python
def query_logs(self, config: dict, time_config: dict, pattern: dict, clustering_field: list) -> dict:
    """并行查询日志"""
    log_map = {}
    with ThreadPoolExecutor() as ex:
        tasks = [
            ex.submit(self.query_logs, config, time_config, pattern, clustering_config["clustering_fields"])
            for pattern in result["new_patterns"]["data"] + result["patterns"]["data"]
        ]
        for feature in as_completed(tasks):
            log_map.update(feature.result())
```

**Section sources**
- [storage.py](file://bkmonitor/metadata/models/storage.py)
- [clustering.py](file://bkmonitor/alarm_backends/service/new_report/handler/clustering.py)

## 系统集成

### 与告警引擎集成

日志系统与告警引擎深度集成，支持告警关联日志查询和告警日志记录。

**告警关联日志查询：**
```python
def get_event_related_info(alert, is_raw=False):
    """
    获取告警关联日志信息
    """
    # 计算查询时间范围（事件开始前5个周期至1个周期后）
    interval = query_config.get("agg_interval", 60)
    start_time = int(source_time) - 5 * interval
    end_time = int(source_time) + interval

    # 执行日志查询（时间戳转换为毫秒）
    records, _ = data_source.query_log(start_time=start_time * 1000, end_time=end_time * 1000, limit=1)
```

**告警日志记录：**
```python
def add_record_alert(self, hit, record):
    """告警产生日志记录"""
    contents = [hit.description]
    # 添加触发条件说明
    if self.alert.strategy and self.alert.strategy["items"][0]["algorithms"]:
        # ... 添加详细触发条件
    record.update({"source_time": source_time, "index": 0, "contents": contents})
    self.log_records.append(record)
```

### 与分析平台集成

日志系统与分析平台集成，支持将日志数据作为分析指标的来源。

**在策略配置中引用日志数据：**
```python
# 在v1.py中，将日志平台作为数据来源
data = {
    "index_set_id": index_set_id,
    "name": metric_field,
    "metric_field": metric_field,
    "result_table_id": result_table_id,
    "scenario_id": scenario_id,
    "scenario_name": scenario_name,
    "data_source_label": DataSourceLabel.BK_LOG_SEARCH,
    "metric_description": _("数据来源：日志平台"),
    "data_type_label": "log",
    "result_table_label": "log",
}
```

**诊断集成：**
```python
# 在diagnostic.md中，定义了日志链路的诊断决策树
Step4. 日志链路关联检验： (注意，仅当存储方案为ES时进行5&6&7&8步骤检验)
Step5. 日志链路-- ES集群状态: if es_storage_infos.当前索引详情信息.index_status != "green" → ES集群状态异常
Step6. 日志链路-- ES集群分片: if es_storage_infos.是否需要进行索引轮转==True → 采集项索引异常,需要进行索引轮转
```

**Section sources**
- [event_related_info.py](file://bkmonitor/bkmonitor/utils/event_related_info.py)
- [v1.py](file://bkmonitor/packages/monitor_web/strategies/resources/v1.py)
- [diagnostic.md](file://bkmonitor/metadata/agents/prompts/diagnostic.md)

## 附录

### 常见问题

**Q: 如何修改已创建的存储集群配置？**
A: 通过`metadata_modify_cluster_info`接口修改存储集群配置，但部分参数（如domain_name）的修改需要运维介入，不支持直接修改。

**Q: 如何处理大量日志数据的查询性能问题？**
A: 建议采取以下措施：
1. 合理配置索引切分策略，避免单个索引过大
2. 使用字段过滤减少查询范围
3. 限制返回结果数量，使用分页查询
4. 对常用查询建立专门的索引

**Q: 如何实现日志数据的长期存储？**
A: 可以通过配置`long_term_storage_settings`参数实现长期存储，将历史数据归档到成本更低的存储系统中。

### 最佳实践

1. **合理规划索引生命周期**：根据业务需求设置合适的保留时间和切分策略
2. **优化字段映射**：只保留必要的字段，避免过度索引
3. **监控存储集群状态**：定期检查ES集群的健康状态和分片情况
4. **使用别名管理索引**：通过别名实现无缝的索引轮转
5. **配置适当的副本数量**：在数据可靠性和存储成本之间取得平衡