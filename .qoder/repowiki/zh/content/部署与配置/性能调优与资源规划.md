# Performance Tuning and Resource Planning

<cite>
**Referenced Files in This Document**   
- [prod.py](file://bkmonitor/config/prod.py)
- [default.py](file://bkmonitor/config/default.py)
- [celery.py](file://bkmonitor/config/celery/celery.py)
- [config.py](file://bkmonitor/config/celery/config.py)
- [web.py](file://bkmonitor/config/role/web.py)
- [worker.py](file://bkmonitor/config/role/worker.py)
- [redis.py](file://bkmonitor/config/tools/redis.py)
- [kafka.py](file://bkmonitor/config/tools/kafka.py)
- [mysql.py](file://bkmonitor/config/tools/mysql.py)
- [elasticsearch.py](file://bkmonitor/config/tools/elasticsearch.py)
- [metrics.py](file://bkmonitor/core/prometheus/metrics.py) - *Updated in recent commit*
- [resources.py](file://bkmonitor/ai_whale/resources/resources.py) - *Updated in recent commit*
- [metrics_reporter.py](file://ai_agent/services/metrics_reporter.py) - *Related to AI metrics implementation*
</cite>

## Update Summary
**Changes Made**   
- Added new section on Unified Observability Metrics for AI Agents
- Updated performance monitoring section to reflect standardized metric naming
- Enhanced documentation on AI小鲸 (AI Whale) service metrics and monitoring
- Added sources for newly analyzed files related to AI agent metrics
- Maintained all existing performance tuning guidance while incorporating AI service observability

## Table of Contents
1. [Introduction](#introduction)
2. [Project Structure](#project-structure)
3. [Core Components](#core-components)
4. [Architecture Overview](#architecture-overview)
5. [Detailed Component Analysis](#detailed-component-analysis)
6. [Dependency Analysis](#dependency-analysis)
7. [Performance Considerations](#performance-considerations)
8. [Troubleshooting Guide](#troubleshooting-guide)
9. [Unified Observability Metrics for AI Agents](#unified-observability-metrics-for-ai-agents)
10. [Conclusion](#conclusion)

## Introduction
This document provides comprehensive guidance for performance tuning and resource planning of the bk-monitor system in production environments. By analyzing resource consumption characteristics of system components—including CPU, memory, disk I/O, and network bandwidth—it offers configuration optimization recommendations for critical components such as Web services, Celery Workers, and database connection pools. The document covers performance optimization methods for caching strategies (Redis), message queues (Kafka/RabbitMQ), database query optimization, indexing strategies, and connection management. It also includes configuration recommendations for monitoring metric collection frequency and data retention policies, along with load testing methods and performance benchmark references to guide capacity planning.

## Project Structure
The bk-monitor project employs a modular design, comprising multiple functional modules such as ai_agent, bkmonitor, api, and apm. Each module serves distinct functions: the ai_agent module handles AI agent-related functionality, the bkmonitor module provides core monitoring capabilities, the api module delivers various API interfaces, and the apm module manages application performance. The clear project structure facilitates maintenance and scalability.

``mermaid
graph TD
A[bk-monitor]
A --> B[ai_agent]
A --> C[bkmonitor]
A --> D[api]
A --> E[apm]
A --> F[config]
A --> G[scripts]
A --> H[wiki]
B --> I[core]
B --> J[llm]
B --> K[services]
C --> L[.devcontainer]
C --> M[.github]
C --> N[ai_whale]
C --> O[alarm_backends]
C --> P[api]
C --> Q[apm]
C --> R[apm_ebpf]
C --> S[bin]
C --> T[bk_dataview]
C --> U[bkm_ipchooser]
C --> V[bkm_space]
C --> W[bkmonitor]
C --> X[blueking]
C --> Y[calendars]
C --> Z[config]
Z --> AA[celery]
Z --> AB[role]
Z --> AC[tools]
Z --> AD[default.py]
Z --> AE[dev.py]
Z --> AF[prod.py]
Z --> AG[stag.py]
```

**Section sources**
- [default.py](file://bkmonitor/config/default.py)
- [prod.py](file://bkmonitor/config/prod.py)

## Core Components
### Web Service
The Web service serves as the front-end entry point for the bk-monitor system, handling user requests and displaying monitoring data. Its performance directly impacts user experience. In `config/role/web.py`, the `CELERYD_CONCURRENCY` parameter can be adjusted to control Celery Worker concurrency, thereby optimizing Web service response speed.

### Celery Worker
Celery Workers handle asynchronous task execution, such as data processing and alert notifications. In `config/celery/config.py`, parameters such as Celery concurrency and task queues can be configured to improve task processing efficiency.

### Database Connection Pool
The database connection pool manages database connections, reducing the overhead of establishing and destroying connections. In `config/tools/mysql.py`, database connection information is obtained through the `get_backend_mysql_settings` function, with connection pool parameters like `POOL_SIZE` and `MAX_OVERFLOW` configured in `default.py`.

### Caching Strategy (Redis)
Redis serves as a caching layer, storing hot data to reduce database access pressure. In `config/tools/redis.py`, Redis connection information is configured, with cache timeout settings such as `CACHE_CC_TIMEOUT` and `CACHE_BIZ_TIMEOUT` set in `default.py`.

### Message Queue (Kafka/RabbitMQ)
Kafka and RabbitMQ serve as message queues, decoupling system components to enhance scalability and reliability. In `config/tools/kafka.py`, Kafka connection information is configured, with message queue parameters like `KAFKA_CONSUMER_GROUP` set in `default.py`.

### Database Query Optimization
Database query optimization is critical for improving system performance. In `config/tools/mysql.py`, database connection information is obtained via `get_backend_mysql_settings`, with query optimization parameters such as `SQL_MAX_LIMIT` configured in `default.py`.

### Monitoring Metric Collection Frequency and Data Retention Strategy
The collection frequency and retention strategy for monitoring metrics directly affect storage costs and query performance. In `default.py`, the `TS_DATA_SAVED_DAYS` parameter sets the retention period for time-series data, while `UNIFY_QUERY_URL` configures the unified query module.

## Architecture Overview
The bk-monitor system adopts a microservices architecture, with components communicating via APIs. The Web service acts as the front-end entry point, receiving user requests and invoking backend services. Backend services—including data collection, data processing, and alert management—communicate asynchronously through message queues. Databases persistently store monitoring data, Redis serves as a caching layer, and Kafka/RabbitMQ function as message queues.

``mermaid
graph TD
A[User] --> B[Web Service]
B --> C[Celery Worker]
B --> D[Database]
B --> E[Redis]
B --> F[Kafka/RabbitMQ]
C --> G[Data Collection]
C --> H[Data Processing]
C --> I[Alert Management]
G --> D
H --> D
I --> D
I --> E
I --> F
```

**Section sources**
- [default.py](file://bkmonitor/config/default.py)
- [celery.py](file://bkmonitor/config/celery/celery.py)

## Detailed Component Analysis
### Web Service Analysis
Web service performance optimization focuses on concurrent processing capability and response speed. In `config/role/web.py`, the `CELERYD_CONCURRENCY` parameter controls Celery Worker concurrency and should be adjusted based on server CPU core count to fully utilize hardware resources.

### Celery Worker Analysis
Celery Worker performance optimization centers on task queue management and concurrent processing. In `config/celery/config.py`, task queue priority and concurrency can be configured to ensure timely processing of high-priority tasks.

### Database Connection Pool Analysis
Database connection pool optimization focuses on connection reuse and management. In `config/tools/mysql.py`, database connection information is obtained via `get_backend_mysql_settings`, with connection pool parameters like `POOL_SIZE` and `MAX_OVERFLOW` configured in `default.py` to reduce connection establishment and destruction overhead.

### Caching Strategy (Redis) Analysis
Redis caching strategy optimization emphasizes cache hit rate and update strategy. In `config/tools/redis.py`, Redis connection information is configured, with cache timeout settings such as `CACHE_CC_TIMEOUT` and `CACHE_BIZ_TIMEOUT` set in `default.py` to ensure timely cache updates.

### Message Queue (Kafka/RabbitMQ) Analysis
Kafka and RabbitMQ message queue optimization focuses on message throughput and latency. In `config/tools/kafka.py`, Kafka connection information is configured, with parameters like `KAFKA_CONSUMER_GROUP` set in `default.py` to ensure efficient message transmission.

### Database Query Optimization Analysis
Database query optimization focuses on query efficiency and indexing strategies. In `config/tools/mysql.py`, database connection information is obtained via `get_backend_mysql_settings`, with optimization parameters like `SQL_MAX_LIMIT` configured in `default.py` to reduce query time.

### Monitoring Metric Collection Frequency and Data Retention Strategy Analysis
Optimization of metric collection frequency and retention strategy balances storage cost and query performance. In `default.py`, `TS_DATA_SAVED_DAYS` sets time-series data retention, while `UNIFY_QUERY_URL` configures the unified query module for efficient data storage and retrieval.

## Dependency Analysis
The bk-monitor system depends on multiple external components, including databases, Redis, Kafka/RabbitMQ, and Elasticsearch. These components' performance directly impacts the overall system performance. Therefore, performance tuning must consider these external dependencies.

``mermaid
graph TD
A[bk-monitor] --> B[MySQL]
A --> C[Redis]
A --> D[Kafka]
A --> E[RabbitMQ]
A --> F[Elasticsearch]
B --> G[Storage]
C --> H[Cache]
D --> I[Message Queue]
E --> J[Message Queue]
F --> K[Search]
```

**Section sources**
- [mysql.py](file://bkmonitor/config/tools/mysql.py)
- [redis.py](file://bkmonitor/config/tools/redis.py)
- [kafka.py](file://bkmonitor/config/tools/kafka.py)
- [elasticsearch.py](file://bkmonitor/config/tools/elasticsearch.py)

## Performance Considerations
### Web Service Performance Considerations
Web service performance optimization focuses on concurrent processing and response speed. Adjust the `CELERYD_CONCURRENCY` parameter based on CPU core count to maximize hardware utilization.

### Celery Worker Performance Considerations
Optimize task queue management and concurrency based on task priority and volume to ensure timely processing of critical tasks.

### Database Connection Pool Performance Considerations
Adjust `POOL_SIZE` and `MAX_OVERFLOW` based on database load to minimize connection establishment and destruction overhead.

### Caching Strategy (Redis) Performance Considerations
Tune cache timeout settings based on data access and update frequency to maintain data freshness and high hit rates.

### Message Queue (Kafka/RabbitMQ) Performance Considerations
Configure message queues based on traffic volume and latency requirements to ensure efficient message delivery.

### Database Query Optimization Performance Considerations
Adjust query optimization parameters based on query frequency and complexity to reduce execution time.

### Monitoring Metric Collection Frequency and Data Retention Strategy Performance Considerations
Balance data importance and query frequency when setting retention periods and query strategies to optimize storage and retrieval efficiency.

## Troubleshooting Guide
### Web Service Troubleshooting
When experiencing performance issues, first verify the `CELERYD_CONCURRENCY` setting, then check CPU and memory utilization to ensure adequate resources.

### Celery Worker Troubleshooting
Investigate task queue configuration, task execution time, and resource consumption to identify bottlenecks in task processing.

### Database Connection Pool Troubleshooting
Verify `POOL_SIZE` and `MAX_OVERFLOW` settings, then examine database connection count and resource usage to ensure effective connection management.

### Caching Strategy (Redis) Troubleshooting
Check cache timeout settings, hit rate, and update strategy to ensure data consistency and freshness.

### Message Queue (Kafka/RabbitMQ) Troubleshooting
Review queue configuration, message throughput, and latency to ensure reliable and efficient message transmission.

### Database Query Optimization Troubleshooting
Examine query optimization parameters, execution plans, and index usage to identify inefficient queries.

### Monitoring Metric Collection Frequency and Data Retention Strategy Troubleshooting
Verify retention settings and query strategies, then assess storage costs and query performance to optimize data management.

## Unified Observability Metrics for AI Agents
This section documents the unified observability metrics implementation for AI小鲸 (AI Whale) services, ensuring consistent monitoring across the platform.

### AI Agent Request Metrics
The system now employs standardized metrics for monitoring AI agent interactions, enabling consistent observability across monitoring and logging platforms.

**AI_AGENTS_REQUESTS_TOTAL**
- **Name**: `ai_agents_requests_total`
- **Type**: Counter
- **Documentation**: "AI小鲸 service invocation statistics"
- **Labels**: `agent_code`, `resource_name`, `status`, `username`, `command`
- **Purpose**: Tracks total invocation count of AI agent services, enabling usage analysis and capacity planning.

**AI_AGENTS_REQUESTS_COST_SECONDS**
- **Name**: `ai_agents_requests_cost_seconds`
- **Type**: Gauge
- **Documentation**: "AI小鲸 service invocation duration statistics"
- **Labels**: `agent_code`, `resource_name`, `status`, `username`, `command`
- **Purpose**: Measures service invocation duration, facilitating performance analysis and SLA monitoring.

### Implementation Details
The metrics are implemented in the `bkmonitor.core.prometheus.metrics` module and integrated into AI Whale resources through the `AIMetricsReporter` class. The `ai_metrics_decorator` is applied to key resource methods to automatically collect invocation and duration metrics.

``mermaid
graph TD
A[AI Whale Resource] --> B[ai_metrics_decorator]
B --> C[AIMetricsReporter]
C --> D[AI_AGENTS_REQUESTS_TOTAL]
C --> E[AI_AGENTS_REQUESTS_COST_SECONDS]
D --> F[Prometheus]
E --> F[Prometheus]
```

**Diagram sources**
- [metrics.py](file://bkmonitor/core/prometheus/metrics.py#L1210-L1220) - *Defined unified metrics*
- [resources.py](file://bkmonitor/ai_whale/resources/resources.py#L25-L30) - *Metrics reporter initialization*
- [resources.py](file://bkmonitor/ai_whale/resources/resources.py#L60-L65) - *Decorator application on resources*

**Section sources**
- [metrics.py](file://bkmonitor/core/prometheus/metrics.py#L1210-L1220) - *Metric definitions*
- [resources.py](file://bkmonitor/ai_whale/resources/resources.py) - *Resource-level integration*
- [metrics_reporter.py](file://ai_agent/services/metrics_reporter.py) - *Metrics reporting implementation*

### Performance Monitoring Integration
These unified metrics should be incorporated into performance monitoring dashboards and alerting rules. Key monitoring scenarios include:
- **Request Volume Monitoring**: Track `ai_agents_requests_total` by `agent_code` and `resource_name` to identify usage patterns.
- **Latency Monitoring**: Monitor `ai_agents_requests_cost_seconds` by `status` to detect performance degradation.
- **Error Rate Calculation**: Calculate error rates using request counts with `status` label to identify reliability issues.
- **User Behavior Analysis**: Analyze metrics by `username` and `command` to understand user interaction patterns.

## Conclusion
This document provides comprehensive guidance for performance tuning and resource planning of the bk-monitor system in production environments. By considering multiple aspects—including Web services, Celery Workers, database connection pools, caching strategies, message queues, database query optimization, and monitoring metric collection—appropriate configuration and optimization can significantly enhance system performance and stability, delivering a superior service experience to users. The recent addition of unified observability metrics for AI agents ensures consistent monitoring across the platform, facilitating holistic performance analysis and capacity planning.