# 数据提取

<cite>
**本文档引用的文件**   
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py)
- [kafka.py](file://bklog/apps/log_databus/handlers/kafka.py)
- [redis_slowlog.py](file://bklog/apps/log_databus/handlers/collector_scenario/redis_slowlog.py)
- [syslog.py](file://bklog/apps/log_databus/handlers/collector_scenario/syslog.py)
- [collector_scenario/base.py](file://bklog/apps/log_databus/handlers/collector_scenario/base.py)
- [models.py](file://bklog/apps/log_databus/models.py)
- [constants.py](file://bklog/apps/log_databus/constants.py)
</cite>

## 目录
1. [引言](#引言)
2. [采集场景基类设计](#采集场景基类设计)
3. [具体采集场景实现](#具体采集场景实现)
4. [采集配置参数定义](#采集配置参数定义)
5. [数据源连接与消息消费模式](#数据源连接与消息消费模式)
6. [采集任务配置与启动](#采集任务配置与启动)
7. [错误处理与重试机制](#错误处理与重试机制)
8. [性能优化策略](#性能优化策略)
9. [API管理采集配置](#api管理采集配置)
10. [结论](#结论)

## 引言
本文档详细描述了蓝鲸日志平台的数据提取机制，重点分析了不同日志源的数据采集实现。文档涵盖了采集场景基类的设计原理、Kafka、Redis慢日志、Syslog等具体场景的实现细节，以及采集配置的参数定义、连接方式、消费模式、错误处理和性能优化策略。

## 采集场景基类设计
采集场景基类`CollectorScenario`是数据采集功能的核心抽象，定义了不同采集场景的通用接口和行为规范。该基类通过工厂模式实现不同采集场景的实例化，支持行日志、段日志、Windows事件、Redis慢日志、Syslog日志和Kafka等多种采集场景。

```mermaid
classDiagram
class CollectorScenario {
+get_instance(collector_scenario_id)
+get_subscription_steps(data_id, params)
+parse_steps(steps)
+get_built_in_config(es_version, etl_config)
+delete_data_id(bk_data_id, data_name)
+update_or_create_data_id(bk_data_id, data_link_id, data_name)
+update_or_create_subscription(collector_config, params)
+_deal_text_public_params(local_params, params)
+_deal_edge_transport_params(local_params, data_link_id)
+_add_labels(local_params, params, collector_config_id)
+_add_ext_meta(local_params, params)
+_handle_collector_config_overlay(local_params, params)
}
class KafkaScenario {
+PLUGIN_NAME
+PLUGIN_VERSION
+CONFIG_NAME
+get_subscription_steps(data_id, params)
+parse_steps(steps)
+get_built_in_config(es_version, etl_config)
}
class RedisSlowLogCollectorScenario {
+PLUGIN_NAME
+PLUGIN_VERSION
+CONFIG_NAME
+get_subscription_steps(data_id, params)
+parse_steps(steps)
+get_built_in_config(es_version, etl_config)
}
class SysLogScenario {
+PLUGIN_NAME
+PLUGIN_VERSION
+CONFIG_NAME
+get_subscription_steps(data_id, params)
+parse_steps(steps)
+get_built_in_config(es_version, etl_config)
}
CollectorScenario <|-- KafkaScenario
CollectorScenario <|-- RedisSlowLogCollectorScenario
CollectorScenario <|-- SysLogScenario
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector_scenario/base.py#L49-L804)
- [kafka.py](file://bklog/apps/log_databus/handlers/collector_scenario/kafka.py#L35-L333)
- [redis_slowlog.py](file://bklog/apps/log_databus/handlers/collector_scenario/redis_slowlog.py#L29-L207)
- [syslog.py](file://bklog/apps/log_databus/handlers/collector_scenario/syslog.py#L29-L264)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector_scenario/base.py#L49-L804)

## 具体采集场景实现

### Kafka采集场景
Kafka采集场景通过`KafkaScenario`类实现，支持从Kafka集群消费消息并进行日志采集。该实现在订阅步骤中配置了Kafka消费者参数，包括主机列表、主题、消费者组、初始偏移量等。

```mermaid
sequenceDiagram
participant 用户 as 用户
participant API as API接口
participant KafkaScenario as Kafka采集场景
participant 节点管理 as 节点管理
participant Kafka消费者 as Kafka消费者
用户->>API : 创建Kafka采集任务
API->>KafkaScenario : 调用get_subscription_steps
KafkaScenario->>KafkaScenario : 构建订阅步骤
KafkaScenario-->>API : 返回订阅配置
API->>节点管理 : 创建订阅
节点管理-->>API : 返回订阅ID
API->>Kafka消费者 : 启动消费者
Kafka消费者->>Kafka集群 : 订阅主题
Kafka集群-->>Kafka消费者 : 推送消息
Kafka消费者->>API : 返回采集结果
```

**图源**
- [kafka.py](file://bklog/apps/log_databus/handlers/collector_scenario/kafka.py#L35-L333)
- [kafka.py](file://bklog/apps/log_databus/handlers/kafka.py#L24-L60)

**节源**
- [kafka.py](file://bklog/apps/log_databus/handlers/collector_scenario/kafka.py#L35-L333)

### Redis慢日志采集场景
Redis慢日志采集场景通过`RedisSlowLogCollectorScenario`类实现，专门用于采集Redis服务器的慢查询日志。该实现配置了Redis主机、端口、密码等连接信息，并通过节点管理插件进行部署。

```mermaid
flowchart TD
A[开始] --> B[配置Redis连接参数]
B --> C[设置主机列表]
C --> D[配置认证信息]
D --> E[构建订阅步骤]
E --> F[添加标签和元数据]
F --> G[处理边缘传输参数]
G --> H[创建或更新订阅]
H --> I[启动采集任务]
I --> J[结束]
```

**图源**
- [redis_slowlog.py](file://bklog/apps/log_databus/handlers/collector_scenario/redis_slowlog.py#L29-L207)

**节源**
- [redis_slowlog.py](file://bklog/apps/log_databus/handlers/collector_scenario/redis_slowlog.py#L29-L207)

### Syslog采集场景
Syslog采集场景通过`SysLogScenario`类实现，支持通过UDP/TCP协议接收Syslog消息。该实现配置了监听协议、端口、IP地址和过滤规则，能够对Syslog消息进行条件过滤。

```mermaid
classDiagram
class SysLogScenario {
+PLUGIN_NAME
+PLUGIN_VERSION
+CONFIG_NAME
+get_subscription_steps(data_id, params)
+parse_steps(steps)
+get_built_in_config(es_version, etl_config)
}
class SyslogFilter {
+conditions : List[Dict]
+logic_op : String
+key : String
+op : String
+value : String
}
SysLogScenario --> SyslogFilter : "包含"
```

**图源**
- [syslog.py](file://bklog/apps/log_databus/handlers/collector_scenario/syslog.py#L29-L264)

**节源**
- [syslog.py](file://bklog/apps/log_databus/handlers/collector_scenario/syslog.py#L29-L264)

## 采集配置参数定义
采集配置参数定义了不同采集场景的配置选项，包括通用参数和特定场景参数。这些参数通过模型类`CollectorConfig`进行存储和管理。

```mermaid
erDiagram
COLLECTOR_CONFIG {
int collector_config_id PK
string collector_config_name
string collector_scenario_id
string category_id
string target_object_type
string target_node_type
json target_nodes
string description
boolean is_active
int data_link_id FK
int bk_data_id FK
string table_id FK
int subscription_id FK
string data_encoding
json collector_config_overlay
int storage_shards_nums
int storage_shards_size
int storage_replies
}
DATA_LINK_CONFIG {
int data_link_id PK
string transfer_cluster_id
string kafka_cluster_id
boolean is_edge_transport
json deploy_options
}
COLLECTOR_CONFIG ||--o{ DATA_LINK_CONFIG : "使用"
```

**图源**
- [models.py](file://bklog/apps/log_databus/models.py#L101-L200)
- [constants.py](file://bklog/apps/log_databus/constants.py#L1-L200)

**节源**
- [models.py](file://bklog/apps/log_databus/models.py#L101-L200)

## 数据源连接与消息消费模式
数据源连接和消息消费模式是采集功能的核心部分，决定了如何与不同数据源建立连接并消费数据。

### Kafka消息消费模式
Kafka消息消费模式实现了从Kafka主题消费消息的完整流程，包括创建消费者、设置偏移量、消费消息和错误处理。

```mermaid
flowchart TD
A[创建Kafka消费者] --> B[获取分区信息]
B --> C[获取起始偏移量]
C --> D[设置消费偏移量]
D --> E[开始消费消息]
E --> F{是否有消息?}
F --> |是| G[处理消息内容]
G --> H[添加到结果列表]
H --> I{达到消息数量?}
I --> |是| J[关闭消费者]
I --> |否| K{是否到达末尾?}
K --> |是| J
K --> |否| E
F --> |否| L[记录无数据]
L --> M[关闭消费者]
```

**图源**
- [kafka.py](file://bklog/apps/log_databus/handlers/kafka.py#L95-L126)
- [kafka_checker.py](file://bklog/apps/log_databus/handlers/check_collector/checker/kafka_checker.py#L88-L128)

**节源**
- [kafka.py](file://bklog/apps/log_databus/handlers/kafka.py#L95-L126)

### 数据源连接方式
数据源连接方式定义了与不同数据源建立连接的机制，包括Kafka、Redis和Syslog等。

```mermaid
classDiagram
class DataLinkConfig {
+data_link_id : int
+transfer_cluster_id : string
+kafka_cluster_id : string
+is_edge_transport : boolean
+deploy_options : json
}
class KafkaConnection {
+server : string
+port : int
+topic : string
+username : string
+password : string
+is_ssl_verify : boolean
+ssl_cafile : string
+ssl_certfile : string
+ssl_keyfile : string
+sasl_mechanism : string
}
class RedisConnection {
+hosts : List[string]
+password : string
+password_file : string
}
class SyslogConnection {
+protocol : string
+host : string
+port : int
+filters : List[Dict]
}
DataLinkConfig --> KafkaConnection : "包含"
DataLinkConfig --> RedisConnection : "包含"
DataLinkConfig --> SyslogConnection : "包含"
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L124-L804)
- [constants.py](file://bklog/apps/log_databus/constants.py#L1-L200)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L124-L804)

## 采集任务配置与启动
采集任务的配置与启动流程涉及多个组件的协同工作，从创建采集配置到最终启动采集任务。

```mermaid
sequenceDiagram
participant 用户 as 用户
participant API as API接口
participant CollectorHandler as 采集处理器
participant 节点管理 as 节点管理
participant 采集插件 as 采集插件
用户->>API : 提交采集配置
API->>CollectorHandler : 创建采集处理器
CollectorHandler->>CollectorHandler : 验证配置参数
CollectorHandler->>CollectorHandler : 创建数据ID
CollectorHandler->>节点管理 : 创建订阅配置
节点管理-->>CollectorHandler : 返回订阅ID
CollectorHandler->>采集插件 : 部署采集插件
采集插件-->>CollectorHandler : 返回部署结果
CollectorHandler->>API : 返回采集任务状态
API-->>用户 : 显示采集任务信息
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L408-L441)
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L293-L322)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L408-L441)

## 错误处理与重试机制
错误处理与重试机制确保了采集任务的稳定性和可靠性，能够应对各种异常情况。

### 错误处理流程
```mermaid
flowchart TD
A[开始采集] --> B{连接成功?}
B --> |否| C[记录连接错误]
C --> D[等待重试间隔]
D --> E{达到最大重试次数?}
E --> |否| F[增加重试计数]
F --> G[重新尝试连接]
G --> B
E --> |是| H[标记任务失败]
H --> I[结束]
B --> |是| J[开始消费消息]
J --> K{消费成功?}
K --> |否| L[记录消费错误]
L --> M[等待重试间隔]
M --> N{达到最大重试次数?}
N --> |否| O[增加重试计数]
O --> P[重新尝试消费]
P --> J
N --> |是| Q[标记任务失败]
Q --> I
K --> |是| R[处理消息]
R --> S{达到批处理大小?}
S --> |否| J
S --> |是| T[提交批处理]
T --> J
```

### 重试机制配置
```mermaid
classDiagram
class DataApiRetryClass {
+stop_max_attempt_number : int
+wait_random_min : int
+wait_random_max : int
+fail_exceptions : List[Exception]
+fail_check_functions : List[Function]
+add_exceptions(exceptions)
+add_fail_check_functions(fail_check_functions)
+retry_on_exception(exception)
+retry_on_result(result)
+create_retry_obj(exceptions, fail_check_functions)
}
class CollectorHandler {
+_pre_start()
+start()
+_pre_stop()
+stop()
+_pre_destroy()
+destroy()
+get_task_status(id_list)
+retry_instances(instance_id_list)
+get_subscription_status()
}
DataApiRetryClass --> CollectorHandler : "用于"
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L408-L441)
- [base.py](file://bklog/apps/api/base.py#L108-L198)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L408-L441)

## 性能优化策略
性能优化策略通过多种技术手段提升采集系统的效率和稳定性。

### 批量处理优化
```mermaid
flowchart TD
A[接收数据] --> B{达到批处理大小?}
B --> |否| C[添加到缓冲区]
C --> D{达到超时时间?}
D --> |否| A
D --> |是| E[处理批处理]
E --> F[清空缓冲区]
F --> A
B --> |是| E
```

### 并发处理优化
```mermaid
classDiagram
class MultiExecuteFunc {
+append(key, func, *args, **kwargs)
+run()
+_execute_single(func, *args, **kwargs)
+_execute_batch(funcs)
}
class CollectorHandler {
+_multi_info_get(use_request)
+bulk_cluster_infos(result_table_list)
}
MultiExecuteFunc --> CollectorHandler : "用于"
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L162-L196)
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L731-L800)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L162-L196)

## API管理采集配置
API管理采集配置提供了对采集任务的全生命周期管理功能。

```mermaid
erDiagram
COLLECTOR_CONFIG ||--o{ TASK_STATUS : "有"
COLLECTOR_CONFIG ||--o{ SUBSCRIPTION : "有"
COLLECTOR_CONFIG ||--o{ DATA_ID : "使用"
COLLECTOR_CONFIG ||--o{ RESULT_TABLE : "生成"
COLLECTOR_CONFIG {
int collector_config_id PK
string collector_config_name
string status
datetime created_at
datetime updated_at
}
TASK_STATUS {
int task_id PK
int collector_config_id FK
string status
json result
datetime created_at
}
SUBSCRIPTION {
int subscription_id PK
int collector_config_id FK
json config
string status
}
DATA_ID {
int bk_data_id PK
string data_name
string status
}
RESULT_TABLE {
string table_id PK
int collector_config_id FK
string status
int storage_cluster_id FK
}
```

**图源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L481-L499)
- [models.py](file://bklog/apps/log_databus/models.py#L101-L200)

**节源**
- [base.py](file://bklog/apps/log_databus/handlers/collector/base.py#L481-L499)

## 结论
本文档详细分析了蓝鲸日志平台的数据提取机制，涵盖了采集场景基类的设计原理、具体采集场景的实现细节、采集配置参数、数据源连接方式、消息消费模式、错误处理机制和性能优化策略。通过这些机制，系统能够高效、稳定地从多种数据源采集日志数据，并提供灵活的配置和管理功能。